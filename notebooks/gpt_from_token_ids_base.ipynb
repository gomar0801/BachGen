{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fd879-fc55-4708-aa1c-c4d41bdd1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Config, \n",
    "    GPT2LMHeadModel, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c06f45-23a5-428f-92f4-75533640e9eb",
   "metadata": {},
   "source": [
    "#### ============= CREATE SAMPLE DATA FILES -> USE THE REAL ONES!============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade8b4-09c5-41c0-9eb8-8ce3a059de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data_files():\n",
    "    \"\"\"Create sample pre-tokenized data files for testing\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Sample vocabulary (token -> id mapping)\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<bos>\": 1,\n",
    "        \"<eos>\": 2,\n",
    "        \"the\": 3, \"a\": 4, \"to\": 5, \"of\": 6, \"and\": 7,\n",
    "        \"in\": 8, \"is\": 9, \"it\": 10, \"for\": 11, \"on\": 12,\n",
    "        \"with\": 13, \"was\": 14, \"as\": 15, \"at\": 16, \"by\": 17,\n",
    "        \"from\": 18, \"up\": 19, \"out\": 20, \"had\": 21, \"but\": 22,\n",
    "    }\n",
    "    \n",
    "    # Extend vocab to 100 tokens for demo\n",
    "    for i in range(23, 100):\n",
    "        vocab[f\"token_{i}\"] = i\n",
    "    \n",
    "    # Save vocabulary\n",
    "    with open(\"data/vocab.json\", \"w\") as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "    \n",
    "    # Generate random pre-tokenized sequences\n",
    "    # Each sequence: [BOS, random tokens, EOS]\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    def generate_sequences(num_sequences, min_len=5, max_len=20):\n",
    "        sequences = []\n",
    "        for _ in range(num_sequences):\n",
    "            seq_len = np.random.randint(min_len, max_len)\n",
    "            # Random token IDs between 3-99 (avoiding special tokens 0,1,2)\n",
    "            tokens = np.random.randint(3, 100, size=seq_len).tolist()\n",
    "            # Add BOS at start and EOS at end\n",
    "            sequence = [1] + tokens + [2]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "    \n",
    "    # Generate train, validation, test sequences\n",
    "    train_sequences = generate_sequences(100)  # 100 sequences for training\n",
    "    valid_sequences = generate_sequences(20)   # 20 for validation  \n",
    "    test_sequences = generate_sequences(20)    # 20 for testing\n",
    "    \n",
    "    # Save to files (one sequence per line, space-separated integers)\n",
    "    def save_sequences(sequences, filepath):\n",
    "        with open(filepath, \"w\") as f:\n",
    "            for seq in sequences:\n",
    "                f.write(\" \".join(map(str, seq)) + \"\\n\")\n",
    "    \n",
    "    save_sequences(train_sequences, \"data/train_ids.txt\")\n",
    "    save_sequences(valid_sequences, \"data/valid_ids.txt\")\n",
    "    save_sequences(test_sequences, \"data/test_ids.txt\")\n",
    "    \n",
    "    print(\"âœ… Created sample data files:\")\n",
    "    print(\"  - data/vocab.json (100 tokens)\")\n",
    "    print(\"  - data/train_ids.txt (100 sequences)\")\n",
    "    print(\"  - data/valid_ids.txt (20 sequences)\")\n",
    "    print(\"  - data/test_ids.txt (20 sequences)\")\n",
    "    \n",
    "    return len(vocab)\n",
    "\n",
    "# Create the sample files\n",
    "VOCAB_SIZE = create_sample_data_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690dd6b-ffab-496b-a8aa-b39b995e9a0b",
   "metadata": {},
   "source": [
    "#### ============= LOAD DATA FROM FILES ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368e6c6-0c14-48b0-9715-24f9653b03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences_from_file(filepath):\n",
    "    \"\"\"Load pre-tokenized sequences from a file\"\"\"\n",
    "    sequences = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            # Each line contains space-separated integers\n",
    "            seq = [int(x) for x in line.strip().split()]\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "# Load vocabulary info\n",
    "with open('data/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "    BOS_TOKEN_ID = vocab.get('<bos>', 1)\n",
    "    EOS_TOKEN_ID = vocab.get('<eos>', 2)\n",
    "    PAD_TOKEN_ID = vocab.get('<pad>', 0)\n",
    "\n",
    "print(f\"\\nðŸ“Š Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "print(f\"Special tokens - BOS: {BOS_TOKEN_ID}, EOS: {EOS_TOKEN_ID}, PAD: {PAD_TOKEN_ID}\")\n",
    "\n",
    "# Load pre-tokenized sequences\n",
    "train_sequences = load_sequences_from_file('data/train_ids.txt')\n",
    "valid_sequences = load_sequences_from_file('data/valid_ids.txt')\n",
    "test_sequences = load_sequences_from_file('data/test_ids.txt')\n",
    "\n",
    "print(f\"\\nðŸ“š Loaded sequences:\")\n",
    "print(f\"  Training: {len(train_sequences)} sequences\")\n",
    "print(f\"  Validation: {len(valid_sequences)} sequences\")\n",
    "print(f\"  Test: {len(test_sequences)} sequences\")\n",
    "\n",
    "# Show example sequence\n",
    "print(f\"\\nExample sequence (first training sequence):\")\n",
    "print(f\"  Token IDs: {train_sequences[0][:10]}...\" if len(train_sequences[0]) > 10 else f\"  Token IDs: {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189dc0b-4ad9-45de-bd31-29563d0a8658",
   "metadata": {},
   "source": [
    "#### ============= DATASET CLASS ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec6a73-bfe0-4dfb-b517-086b222edcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTokenizedDataset(Dataset):\n",
    "    \"\"\"Dataset for pre-tokenized integer sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, block_size=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: List of lists, each inner list is a sequence of token IDs\n",
    "            block_size: Maximum sequence length (will chunk longer sequences)\n",
    "        \"\"\"\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "        \n",
    "        # Concatenate all sequences (they already have EOS tokens)\n",
    "        all_tokens = []\n",
    "        for seq in sequences:\n",
    "            all_tokens.extend(seq)\n",
    "        \n",
    "        # Chunk into fixed-size blocks for language modeling\n",
    "        # This is standard for GPT-2 training\n",
    "        for i in range(0, len(all_tokens) - block_size + 1, block_size):\n",
    "            chunk = all_tokens[i:i + block_size]\n",
    "            if len(chunk) == block_size:  # Only keep full blocks\n",
    "                self.examples.append(chunk)\n",
    "        \n",
    "        print(f\"  Created {len(self.examples)} chunks of size {block_size} from {len(sequences)} sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For language modeling, we return the same sequence as both input and label.\n",
    "        HuggingFace's GPT2LMHeadModel internally shifts the labels to predict next tokens.\n",
    "        \n",
    "        Internally, the model does:\n",
    "        - Input: [A, B, C, D] \n",
    "        - Predicts: [B, C, D, E]\n",
    "        - Loss computed on positions 1 to n (ignores position 0 of labels)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.examples[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40c051-daf7-4584-95b0-d2a4601001a6",
   "metadata": {},
   "source": [
    "#### ============= DATA COLLATOR ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946df39-43c7-40ce-8e8d-1cd106793f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDataCollator:\n",
    "    \"\"\"Simple collator for batching post-tokenized sequences\"\"\"\n",
    "#     Dataset outputs individual examples:\n",
    "# [1, 23, 45, 67, 89, 12, 34, 56]  # Shape: [128]\n",
    "# [1, 34, 56, 78, 90, 23, 45, 67]  # Shape: [128]\n",
    "# [1, 67, 89, 12, 34, 56, 78, 90]  # Shape: [128]\n",
    "# [1, 45, 67, 89, 12, 34, 56, 78]  # Shape: [128]\n",
    "\n",
    "# â†“ Collator stacks them â†“\n",
    "\n",
    "# Batch tensor for GPU/model:\n",
    "# [[1, 23, 45, 67, 89, 12, 34, 56],\n",
    "#  [1, 34, 56, 78, 90, 23, 45, 67],\n",
    "#  [1, 67, 89, 12, 34, 56, 78, 90],\n",
    "#  [1, 45, 67, 89, 12, 34, 56, 78]]  # Shape: [4, 128]\n",
    "    def __call__(self, examples):\n",
    "        # Stack all input_ids and labels\n",
    "        input_ids = torch.stack([ex[\"input_ids\"] for ex in examples])\n",
    "        labels = torch.stack([ex[\"labels\"] for ex in examples])\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d7532-1dfd-4644-9c25-014524061bfc",
   "metadata": {},
   "source": [
    "#### ============= CONFIGURATION ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c10bd6-8df7-4e63-a0a7-dced9289004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128  # Maximum sequence length for both model and data\n",
    "\n",
    "print(\"\\nðŸ¤– Configuring model...\")\n",
    "\n",
    "# Small GPT-2 config for testing\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_LENGTH,  # Maximum position embeddings\n",
    "    n_ctx=MAX_LENGTH,        # Context size (same as n_positions)\n",
    "    n_embd=128,              # Hidden size (very small for testing)\n",
    "    n_layer=2,               # Number of layers (very small for testing)\n",
    "    n_head=2,                # Number of attention heads\n",
    "    bos_token_id=BOS_TOKEN_ID,\n",
    "    eos_token_id=EOS_TOKEN_ID,\n",
    "    pad_token_id=PAD_TOKEN_ID,\n",
    ")\n",
    "\n",
    "# Initialize model from scratch\n",
    "model = GPT2LMHeadModel(config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51623424-a020-459c-a344-a0ac474697f3",
   "metadata": {},
   "source": [
    "#### ============= PREPARE DATASETS ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bd776-bf07-48ee-a76b-7c6e7713f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BLOCK_SIZE = MAX_LENGTH  # Use same size for data chunks!\n",
    "\n",
    "print(f\"\\nðŸ“¦ Preparing datasets with block size {BLOCK_SIZE}...\")\n",
    "train_dataset = PostTokenizedDataset(train_sequences, block_size=BLOCK_SIZE)\n",
    "valid_dataset = PostTokenizedDataset(valid_sequences, block_size=BLOCK_SIZE)\n",
    "test_dataset = PostTokenizedDataset(test_sequences, block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce514b3a-25f2-49fa-83c7-880e9fd2ee37",
   "metadata": {},
   "source": [
    "#### ============= TRAINING ARGUMENTS ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb477e-b0f4-4832-817a-7bb27571acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-pretokenized\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=20,              # Few epochs for testing\n",
    "    per_device_train_batch_size=4,   # Small batch for testing\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,   \n",
    "    \n",
    "    # Learning rate schedule\n",
    "    learning_rate=5e-4,              \n",
    "    warmup_steps=10,                 \n",
    "    weight_decay=0.01,               \n",
    "    \n",
    "    # Evaluation and saving - using correct v4.55 syntax\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,              \n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    dataloader_num_workers=0,        \n",
    "    \n",
    "    # Disable unwanted features\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac01f2d-252a-4044-bc79-225a15138e03",
   "metadata": {},
   "source": [
    "#### ============= TRAIN MODEL ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8d84e-eb87-489d-a3bc-3c24ff87f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=SimpleDataCollator(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af49829-83b0-4fee-81aa-d9f2f7821209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= EVALUATE ON VALIDATION =============\n",
    "print(\"\\n Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation loss: {eval_results['eval_loss']:.4f}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "import math\n",
    "perplexity = math.exp(eval_results['eval_loss']) if eval_results['eval_loss'] < 20 else float(\"inf\")\n",
    "print(f\"Validation perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# ============= EVALUATE ON TEST =============\n",
    "print(\"\\n Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Test loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "test_perplexity = math.exp(test_results['eval_loss']) if test_results['eval_loss'] < 20 else float(\"inf\")\n",
    "print(f\"Test perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06faad77-61df-483e-b0b3-9ed3052cd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============= SAVE MODEL =============\n",
    "trainer.save_model(\"./final_model\")\n",
    "print(\"\\nðŸ’¾ Model saved to ./final_model\")\n",
    "\n",
    "\n",
    "# ============= SIMPLE GENERATION TEST =============\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Start with BOS token\n",
    "    input_ids = torch.tensor([[BOS_TOKEN_ID]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate 15 tokens\n",
    "    max_length = 15\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Sample from top-k tokens for more interesting generation\n",
    "        top_k = 5\n",
    "        top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(top_k_logits, dim=-1)\n",
    "        next_token = top_k_indices[torch.multinomial(probs, 1)].item()\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token == EOS_TOKEN_ID:\n",
    "            break\n",
    "            \n",
    "        # Create new token on same device\n",
    "        new_token = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "        input_ids = torch.cat([input_ids, new_token], dim=1)\n",
    "    \n",
    "    generated = input_ids[0].cpu().tolist()\n",
    "    print(f\"Generated token IDs: {generated}\")\n",
    "    print(f\"Length: {len(generated)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872edaa8-bd73-450a-9b89-dcb4da2890b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
